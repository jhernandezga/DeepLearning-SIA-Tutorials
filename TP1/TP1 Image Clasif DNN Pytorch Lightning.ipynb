{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGntICmaak1",
        "outputId": "1ceece0c-013d-46a0-ae1c-edf9f81c58a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pytorch-lightning in /users/jorghern70/.local/lib/python3.10/site-packages (2.0.9.post0)\n",
            "Requirement already satisfied: lightning-utilities>=0.7.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from pytorch-lightning) (0.9.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from pytorch-lightning) (1.2.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /users/jorghern70/.local/lib/python3.10/site-packages (from pytorch-lightning) (1.26.0)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from pytorch-lightning) (4.8.0)\n",
            "Requirement already satisfied: torch>=1.11.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from pytorch-lightning) (2.0.1)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from pytorch-lightning) (2023.9.2)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/lib/python3/dist-packages (from pytorch-lightning) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from pytorch-lightning) (4.66.1)\n",
            "Requirement already satisfied: packaging>=17.1 in /users/jorghern70/.local/lib/python3.10/site-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.25.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /users/jorghern70/.local/lib/python3.10/site-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (3.8.5)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (2.14.3)\n",
            "Requirement already satisfied: filelock in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (8.5.0.96)\n",
            "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.9)\n",
            "Requirement already satisfied: networkx in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: jinja2 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.4.0.1)\n",
            "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->pytorch-lightning) (59.6.0)\n",
            "Requirement already satisfied: lit in /users/jorghern70/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (17.0.1)\n",
            "Requirement already satisfied: cmake in /users/jorghern70/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.27.5)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /users/jorghern70/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.9.2)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /users/jorghern70/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (4.0.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /users/jorghern70/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /users/jorghern70/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (23.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.1.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.3)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: torch-summary in /users/jorghern70/.local/lib/python3.10/site-packages (1.4.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install torch-summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gK84XoasrgAC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu-IgiEnanUm"
      },
      "source": [
        "# Pytorch-Lightning : Training made Easier\n",
        "\n",
        "Time : 4 hours\n",
        "\n",
        "In the Tutorial session, we used PyTorch to train different models for Binary Classification. In the tutorial, few things were done :\n",
        "\n",
        "\n",
        "*   We created a Training/Testing Loop and trained our models\n",
        "*   We created a Trainer Class to gather all loops to perform the Training/Testing.\n",
        "\n",
        "\n",
        "As you have seen, writing the training and testing loop can quickly be indigest. One can get easily lost.\n",
        "\n",
        "Let us introduce you Pytorch Lightning\n",
        "\n",
        "<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a88de56e65d2ea6bc203ce178a1cecbe9b50a0ac/68747470733a2f2f6769746875622e636f6d2f5079546f7263684c696768746e696e672f7079746f7263682d6c696768746e696e672f7261772f312e342e392f646f63732f736f757263652f5f7374617469632f696d616765732f6c6f676f2e706e67\">\n",
        "\n",
        "\n",
        "Pytorch lightning will handle a lot of things for you. It creates a Trainer which is a Code Management trick used by many companies (Meta, Google..) in order to get much more digest code.\n",
        "\n",
        "\n",
        "More Information on : https://www.pytorchlightning.ai/\n",
        "\n",
        "Goal of this lab :\n",
        "\n",
        "* Use Pytorch Lightning for Training\n",
        "* Learn to use Pytorch-Lightning\n",
        "* Do classification on MNIST, CIFAR-10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_cnXBS7boC7"
      },
      "source": [
        "# I - Classify Numbers using Lightning\n",
        "\n",
        "In this part, we are interested in classifying digit images ranging from 0 to 9. \n",
        "We will use the Lightning framework for code management. What's interesting about Lightning is that you can plug in your Torch modules without any modification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xBCfzUJcJO9"
      },
      "source": [
        "## a - LightningDataModule : MNIST\n",
        "\n",
        "As you have seen in the Tutorial, you need to create your Dataset Class.\n",
        "\n",
        "As a reminder :    \n",
        " The Dataset class returns one sample of your dataset at a time. The main methods of the Dataset class are \n",
        "\n",
        "*   __getitem__ : which fetched a sample at a given index\n",
        "*   __len__ : which returns the len of the total dataset\n",
        "\n",
        "The Dataset is loaded into a DataLoader. That Dataloader is then used to **fetch and send data as batches** for your Model.\n",
        "\n",
        "You will see that using Lightning makes things clearer. LightningDataModule allows you to write cleaner Code and fit easily your data to your model.\n",
        "\n",
        "You can always, use the basic Pytorch Dataloader in a separate code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaDUVAFvqp5h"
      },
      "source": [
        "We are first going to work with the MNIST dataset. There is already a MNIST class provided in the Torchvision library, so we don't have to code the Dataset implementation ourself.\n",
        "* Fill in the blanks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrxvDv_8r9mU"
      },
      "source": [
        "### Exploratory Data Analysis : Discovering the Data\n",
        "\n",
        "Before tackling the classification task it is essential to explore the data we are working on, and understand its specifics.\n",
        "Perform an Exploratory Data Analysis (EDA) on the MNIST Dataset :\n",
        "\n",
        "1.   What type of Data do you have ? (Images, Texts, Sound..) <br>\n",
        "    Images 28x28pixels representing a number and their respective labels in int format   \n",
        "2.   How many Data do you have ? <br>\n",
        "    60000 training,  10000 testing\n",
        "3.   What's in a sample (1 element of the Dataset)<br>\n",
        "    (Image(28x28 PIL), Label(Int))\n",
        "4.   Is the Dataset umbalanced ?\n",
        "\n",
        "5.   What's the shape of any input sample ?<br>\n",
        "    28x28\n",
        "\n",
        "6.   ....\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EDrwvhJAsEz_"
      },
      "outputs": [],
      "source": [
        "# Loading the Training Split of MNIST Dataset\n",
        "dataset_train  = MNIST('', train=True, download=True)\n",
        "dataset_test = MNIST('', train=False, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vwYTFrhtZ43",
        "outputId": "f3074ec1-6863-4aa4-aac6-540ae2ade880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of the train split :  60000\n",
            "Length of the test split :  10000\n"
          ]
        }
      ],
      "source": [
        "# TODO : What's the length of the train and test split ?\n",
        "print(\"Length of the train split : \", len(dataset_train))\n",
        "print(\"Length of the test split : \", len(dataset_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF-0t0ETt1Fa",
        "outputId": "d8fbc436-4601-4d53-cee5-e046526f8522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<PIL.Image.Image image mode=L size=28x28 at 0x7FCCA0161B40>, 5)\n"
          ]
        }
      ],
      "source": [
        "# TODO : Retrieve one sample of the Dataset.\n",
        "sample = dataset_train[0]\n",
        "\n",
        "# TODO : What is in a sample ? Print the sample to understand\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "pNqZ8DmhuDU0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABAElEQVR4nGNgGMyAWUhIqK5jvdSy/9/rGRgYGFhgEnJsVjYCwQwMDAxPJgV+vniQgYGBgREqZ7iXH8r6l/SV4dn7m8gmCt3++/fv37/Htn3/iMW+gDnZf/+e5WbQnoXNNXyMs/5GoQoxwVmf/n9kSGFiwAW49/11wynJoPzx4YIcRlyygR/+/i2XxCWru+vv32nSuGQFYv/83Y3b4p9/fzpAmSyoMnohpiwM1w5h06Q+5enfv39/bcMiJVF09+/fv39P+mFKiTtd/fv3799jgZiBJLT69t+/f/8eDuDEkDJf8+jv379/v7Ryo4qzMDAwMAQGMjBc3/y35wM2V1IfAABFF16Aa0wAOwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<PIL.Image.Image image mode=L size=28x28 at 0x7FCCA0161B40>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO : Plot the image in the sample. Does it correspond to the second element of the sample ?\n",
        "\n",
        "plt.imshow(np.asarray(sample[0]))\n",
        "display(sample[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QUYEXO0TuZ24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(28, 28)\n"
          ]
        }
      ],
      "source": [
        "# TODO : What's the shape of the input image.\n",
        "shape = sample[0].size\n",
        "print(shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBDDrYrKlr8X"
      },
      "source": [
        "### Lightning DataModule : Dataset and DataLoader Embedded \n",
        "\n",
        "Pytorch Lightning introduces a new way to define and organize our dataset via \"data module\". They neatly encompass our training, validation and testing datasets and provide their dataloaders as well.\n",
        "\n",
        "Have a look at : https://pytorch.org/vision/stable/datasets.html\n",
        "\n",
        "We are now going to define a `LightningDataModule` for the MNIST dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HsUhr7BihqxX"
      },
      "outputs": [],
      "source": [
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        self.data_dir = ''\n",
        "        self.batch_size_train, self.batch_size_valid, self.batch_size_test = 32,32,32\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # This method is used to download beforehand the dataset if needed.\n",
        "        # TODO : Load the train and test dataset.\n",
        "        MNIST(self.data_dir, train=True, download=True)\n",
        "        MNIST(self.data_dir, train=False, download=True)\n",
        "        \n",
        "\n",
        "    def setup(self, stage):\n",
        "        # We need to setup our module. We have \n",
        "        #  1. A training set that we will **fit** our model to\n",
        "        #  2. A testing set used to **test** our models prediction.\n",
        "        \n",
        "        # The stage variable corresponds to those two steps : \n",
        "        # stage in {fit, test, None}\n",
        "\n",
        "        # First stage is 'fit' (or None)\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            # We create a validation split to watch the training.\n",
        "\n",
        "            # TODO : Which dataset do we load for training ?\n",
        "            mnist_dataset = MNIST(self.data_dir, train=True, transform=self.transform)\n",
        "            train_size = int(0.8 * len(mnist_dataset))\n",
        "            test_size = len(mnist_dataset) - train_size\n",
        "            # TODO : Load the datasets as attributes of the Module. Don't forget you validation split\n",
        "            self.mnist_train, self.mnist_valid =  torch.utils.data.random_split(mnist_dataset, [train_size, test_size]) \n",
        "\n",
        "        # Second stage is 'test' \n",
        "        if stage == \"test\" or stage is None:\n",
        "\n",
        "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "            # Question : What additional set can we create ? Why ?\n",
        "            #Inference or Prediction Dataset: This is the dataset on which you want to make predictions or inference using your trained model\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # TODO : Now create your Training DataLoader\n",
        "        return DataLoader(self.mnist_train, batch_size=self.batch_size_train, shuffle=True,)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # TODO : Now create your Validation DataLoader\n",
        "        return DataLoader(self.mnist_valid, batch_size=self.batch_size_valid, shuffle=False)\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        # TODO : Now create your Testing DataLoader\n",
        "        return DataLoader(self.mnist_test, batch_size=self.batch_size_test, shuffle=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71GzlDYqj5xY"
      },
      "source": [
        "## b - LightningModule :  MNIST Classifier \n",
        "\n",
        "Similarly to `LightningDataModule`, pytorch Lightning provides a `LightningModule` that gathers every functions needed for the training and testing of our pytorch model.\n",
        " \n",
        "Design a model to perform Classification using the `LightningModule` class layout. Again, ask yourself the following questions: \n",
        "* What task is it ?<br>\n",
        "classification of number\n",
        "* What data do I have ?<br>\n",
        "Images and their labels\n",
        "* What learning rate should I use ? <br>\n",
        "?????????????\n",
        "* What could be my loss ? Why ?<br>\n",
        "Multiclass classification --> softmax\n",
        "* What non-linearity should I use ?<br>\n",
        "ReLU, LeakyReLu, tanh at the end\n",
        "* How do I evaluate my model ? (TorchMetrics is your friend)<br>\n",
        "Accuracy, F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "XUBuWwX3iypq"
      },
      "outputs": [],
      "source": [
        "class MNISTClassifier(pl.LightningModule):\n",
        "    def __init__(self,in_channels = 1, output_shape = 10, learning_rate = 1e-3):\n",
        "        super(MNISTClassifier,self).__init__()\n",
        "        # what is the output_shape of your model ?\n",
        "        self.output_shape = output_shape\n",
        "        self.learning_rate = learning_rate\n",
        "        self.index_test = 0\n",
        "        self.acc = 0\n",
        "        self.save_hyperparameters()\n",
        "        # TODO : Define your model here, be careful, your model will be an instance of the class. Watch  out for the input data.\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1,padding=1),  #28x28x1 -> 28x28x32\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                          #28x28x32 -> 14x14x32\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1,padding = 1),         #14x14x32 -> 14x14x64\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),                          #14x14x64 -> 7x7x64\n",
        "            nn.Flatten(),                                                    #7x7x64 -> 3136\n",
        "            nn.Linear(3136, 128),                                           #3136 -> 128\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, output_shape),                                              #128 -> 10\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        # TODO : What would be the forward steps of this classifier ? Return the output of our classifier given an input batch x.\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # TODO : Choose your optimizer : https://pytorch.org/docs/stable/optim.html\n",
        "        optimizer = torch.optim.Adam(self.model.parameters(), lr= self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # TODO : Define your Training Step\n",
        "        # This method is pretty much similar to what your did in the Tutorial to train your model.\n",
        "        images,labels = batch     \n",
        "\n",
        "        prediction = self.forward(images)\n",
        "        loss = F.cross_entropy(prediction, labels)\n",
        "        acc = torchmetrics.functional.accuracy(prediction, labels,task='multiclass', num_classes=self.output_shape)\n",
        "        # Don't remove the next line, you will understand why later\n",
        "        self.log('train_acc', acc)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # TODO : Define your Validation Step\n",
        "        # What is the difference between the Training and the Validation Step ?\n",
        "        images,labels = batch\n",
        "        prediction = self.forward(images)\n",
        "        loss = F.cross_entropy(prediction, labels)\n",
        "        acc = torchmetrics.functional.accuracy(prediction, labels,task='multiclass', num_classes=self.output_shape)\n",
        "        # Don't remove the next line, you will understand why later\n",
        "        self.log('val_acc', acc)\n",
        "        self.log('val_loss', loss)\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # TODO : Define your Test Step\n",
        "        # What is the difference between the Training, Validation and Test Step ?\n",
        "        images,labels = batch\n",
        "        prediction = self.forward(images)\n",
        "        loss = F.cross_entropy(prediction, labels)\n",
        "        self.index_test += 1\n",
        "        self.acc += torchmetrics.functional.accuracy(prediction, labels,task='multiclass', num_classes=self.output_shape) # We accumulate every accuracy\n",
        "        # Don't remove the next line, you will understand why later\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_acc', self.acc)\n",
        "\n",
        "    def test_epoch_start(self):\n",
        "        self.acc = 0\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        self.acc = self.acc/self.index_test\n",
        "        self.log('Final Accuracy', self.acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TV-SLU6kWV7"
      },
      "source": [
        "## c - Did you say Train ?\n",
        "\n",
        "Let's train the model. \n",
        "\n",
        "We create our so called Trainer that will handle a lot of thing for us. Lightning trainer is full of interesting assets that helps you for your training. The lightning trainer is a much more evolved Trainer than the one in the Tutorial.\n",
        "\n",
        "To get a glance of what Lightning Trainer can give :\n",
        "https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html\n",
        "\n",
        "It also easily lets us using TensorBoard. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/users/jorghern70/.local/lib/python3.10/site-packages')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "L9wyr4pjj5R-"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type       | Params\n",
            "-------------------------------------\n",
            "0 | model | Sequential | 421 K \n",
            "-------------------------------------\n",
            "421 K     Trainable params\n",
            "0         Non-trainable params\n",
            "421 K     Total params\n",
            "1.687     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 1500/1500 [00:36<00:00, 40.90it/s, v_num=1]       "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 1500/1500 [00:36<00:00, 40.88it/s, v_num=1]\n"
          ]
        }
      ],
      "source": [
        "import tensorboard\n",
        "tb_logger = pl.loggers.TensorBoardLogger(\"introduction to Lightning\")\n",
        "\n",
        "dm = MNISTDataModule()\n",
        "model = MNISTClassifier(output_shape=10)\n",
        "\n",
        "trainer = pl.Trainer(max_epochs=5,accelerator='gpu',logger=tb_logger,enable_checkpointing=True)\n",
        "trainer.fit(model, dm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej56Kf9Zkefo"
      },
      "source": [
        "Oh it's training ! Happy ? Easy ? Let's test the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaWmGYVTjRz5"
      },
      "source": [
        "## d - Did you say Test ?\n",
        "\n",
        "For testing, well it's pretty easy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "idVeeYZKVvSS"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0: 100%|██████████| 313/313 [00:04<00:00, 67.41it/s]\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "       Test metric             DataLoader 0\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
            "     Final Accuracy         0.9856230020523071\n",
            "        test_acc            154.15789794921875\n",
            "        test_loss           1.4756395816802979\n",
            "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'test_loss': 1.4756395816802979,\n",
              "  'test_acc': 154.15789794921875,\n",
              "  'Final Accuracy': 0.9856230020523071}]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.test(model, datamodule=dm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_fuzVPMNX5"
      },
      "source": [
        "## e - TensorBoard\n",
        "\n",
        "TensorBoard is a really useful tool. Indeed, it let's you register interesting values during training and plot them INTERACTIVELY. You might have seen a self.log line in the Validation and Training steps. \n",
        "The self.log saves the loss value into a TensorBoard readable file. We can also add images or other values using self.log\n",
        "\n",
        "In fact, look at the checkpoint created by the training. You might see 3 files :\n",
        "* Checkpoint\n",
        "* event.out....\n",
        "* hparam.yaml\n",
        "\n",
        "Let's open tensorboard to see how the training was. Tensorboard is loadable using magic_python commands.\n",
        "More info on TensorBoard : https://www.tensorflow.org/tensorboard/get_started\n",
        "\n",
        "Another popular alternative to Tensorboard, also usable with pytorch lightning, is \"Weight and Biases\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzLzFpFdkdoM"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"/content/introduction to Lightning/default/version_0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCx4XtQojqBf"
      },
      "source": [
        "Pytorch Lightning can be used along PyTorch. We encourage you to use PyTorch Lightning during your Lab Sessions and Career as it simplifies a lot of things for you (MultiGPU, Learning Rate Decay...)\n",
        "\n",
        "<img src='https://c.tenor.com/VyApQ-jWyV0AAAAC/happy-borat.gif'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq2vIzXbyXtk"
      },
      "source": [
        "# II - Classify Objects using Lightning \n",
        "\n",
        "We will now turn to another classification task, this time object classification. We use the CIFAR-10 dataset which is made of 10 different classes of objects which we aim to distinguish. This part of the lab will be less restricted and more free. You now should have a sense of how to use the Lightning Framework. \n",
        "Be creative.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVi6aj84y97g"
      },
      "source": [
        "## a - Baseline : Creating your own Model\n",
        "\n",
        "In this first section, we will create a Simple Model and perform all steps from part 1 with the needed changes.\n",
        "\n",
        "*   **What's your final accuracy ?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LodackDWzb9k"
      },
      "source": [
        "### i - DataModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La8kvImxzgaf"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUes-3TPFhW8"
      },
      "outputs": [],
      "source": [
        "# TODO : EDA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hJKuGjwFHxE"
      },
      "outputs": [],
      "source": [
        "# TODO : Create your DataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ykxzmaMzhEr"
      },
      "source": [
        "### ii - Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZWj3Qp8zgej"
      },
      "outputs": [],
      "source": [
        "# TODO : Create your Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjDUQJBhzloB"
      },
      "source": [
        "### iii - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1-7v8j9zoAJ"
      },
      "outputs": [],
      "source": [
        "# TODO : Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8RmPe1ozoxq"
      },
      "source": [
        "### iv - Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXDiuQOMzoxr"
      },
      "outputs": [],
      "source": [
        "# TODO : Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls7EK0Az2YFr"
      },
      "source": [
        "\n",
        "**Does your model perform well on the CIFAR Dataset ?**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfcW6jcizB1I"
      },
      "source": [
        "## b - The OG Model : Finetuning a Model\n",
        "\n",
        "If your model performed well on the CIFAR-10 Dataset, congrats. But let's achieve better results. For industrial works, we often pretrain a model on a large dataset (ImageNet or internal Dataset), and then fine-tune the model on the Dataset of interest.\n",
        "\n",
        "* **What's the intuition behind fine-tuning ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qxg-fff7Ofw"
      },
      "source": [
        "### i - Importing a Pretrained Model\n",
        "\n",
        "We will import a ConvNext model. Why ? It's said to be a really good backbone that competes with the Transformer models. Let's load the model. \n",
        "We are going to use TorchSummary to print what the size of the inputs and outputs are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIgUs9Fd7FW7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torchvision\n",
        "model = torchvision.models.convnext_small(weights='DEFAULT')\n",
        "\n",
        "# TODO : Using torchsummary, print a summary of the model\n",
        "from torchsummary import summary\n",
        "summary(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x29gjtvd94Jr"
      },
      "outputs": [],
      "source": [
        "# TODO : Using torchsummary, send an image of the same size as a sample of CIFAR-10\n",
        "summary(model, ...) # ... = input shape as a tuple (C,H,W)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIpoh9qP-Nll"
      },
      "source": [
        "* **What is the output size of the model ?**\n",
        "* **What will be the issue of using this model as is to perform classification on the CIFAR-10 Dataset ?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XBym9hsB2Z1"
      },
      "outputs": [],
      "source": [
        "# TODO : According to your answer to the previous questions, perform the changes.\n",
        "# You can access each layers using model.name_of_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcainrOT51rP"
      },
      "source": [
        "### ii - DataModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3WlWJvy51rP"
      },
      "outputs": [],
      "source": [
        "# TODO : EDA \n",
        "\n",
        "# TODO : Create your DataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aUiAPtd51rP"
      },
      "source": [
        "### iii - Module\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0zUy7CJ51rP"
      },
      "outputs": [],
      "source": [
        "# TODO : Create your Module\n",
        "\n",
        "# Careful : How should your learning rate be ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwPYkPT551rQ"
      },
      "source": [
        "### iv - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GdvMUH-51rQ"
      },
      "outputs": [],
      "source": [
        "# TODO : Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEJLdQuZ51rQ"
      },
      "source": [
        "### v - Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvxjzGdV51rQ"
      },
      "outputs": [],
      "source": [
        "# TODO : Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USu9oo2wCV2j"
      },
      "source": [
        "\n",
        "* **What is your final accuracy ?**\n",
        "* **Is Fine Tuning a model better than creating your own model ?**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Introduction to Pytorch Lightning",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

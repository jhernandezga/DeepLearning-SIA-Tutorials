{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaGntICmaak1",
        "outputId": "1ceece0c-013d-46a0-ae1c-edf9f81c58a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.9.post0-py3-none-any.whl (727 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m727.7/727.7 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting fsspec[http]>2021.06.0\n",
            "  Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 KB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=17.1 in /users/jorghern70/.local/lib/python3.10/site-packages (from pytorch-lightning) (23.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from pytorch-lightning) (2.0.1)\n",
            "Collecting tqdm>=4.57.0\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 KB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /usr/lib/python3/dist-packages (from pytorch-lightning) (1.21.5)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from pytorch-lightning) (4.8.0)\n",
            "Collecting lightning-utilities>=0.7.0\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/lib/python3/dist-packages (from pytorch-lightning) (5.4.1)\n",
            "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning) (2.25.1)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
            "  Downloading aiohttp-3.8.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.99)\n",
            "Requirement already satisfied: filelock in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (10.2.10.91)\n",
            "Requirement already satisfied: networkx in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.1)\n",
            "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch>=1.11.0->pytorch-lightning) (1.9)\n",
            "Requirement already satisfied: jinja2 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (2.0.0)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /users/jorghern70/.local/lib/python3.10/site-packages (from torch>=1.11.0->pytorch-lightning) (10.9.0.58)\n",
            "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->pytorch-lightning) (59.6.0)\n",
            "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.11.0->pytorch-lightning) (0.37.1)\n",
            "Requirement already satisfied: cmake in /users/jorghern70/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (3.27.5)\n",
            "Requirement already satisfied: lit in /users/jorghern70/.local/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.11.0->pytorch-lightning) (17.0.1)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.4.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.7/225.7 KB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting attrs>=17.3.0\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 KB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 KB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4.0,>=2.0\n",
            "  Downloading charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.8/201.8 KB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 KB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch>=1.11.0->pytorch-lightning) (2.0.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/lib/python3/dist-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning) (3.3)\n",
            "Installing collected packages: tqdm, multidict, lightning-utilities, fsspec, frozenlist, charset-normalizer, attrs, async-timeout, yarl, aiosignal, aiohttp, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.1.0 charset-normalizer-3.2.0 frozenlist-1.4.0 fsspec-2023.9.2 lightning-utilities-0.9.0 multidict-6.0.4 pytorch-lightning-2.0.9.post0 torchmetrics-1.2.0 tqdm-4.66.1 yarl-1.9.2\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting torch-summary\n",
            "  Downloading torch_summary-1.4.5-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torch-summary\n",
            "Successfully installed torch-summary-1.4.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-lightning\n",
        "!pip install torch-summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gK84XoasrgAC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Matplotlib is building the font cache; this may take a moment.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import transforms\n",
        "from pytorch_lightning import loggers as pl_loggers\n",
        "import pytorch_lightning as pl\n",
        "import matplotlib.pyplot as plt\n",
        "import torchmetrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu-IgiEnanUm"
      },
      "source": [
        "# Pytorch-Lightning : Training made Easier\n",
        "\n",
        "Time : 4 hours\n",
        "\n",
        "In the Tutorial session, we used PyTorch to train different models for Binary Classification. In the tutorial, few things were done :\n",
        "\n",
        "\n",
        "*   We created a Training/Testing Loop and trained our models\n",
        "*   We created a Trainer Class to gather all loops to perform the Training/Testing.\n",
        "\n",
        "\n",
        "As you have seen, writing the training and testing loop can quickly be indigest. One can get easily lost.\n",
        "\n",
        "Let us introduce you Pytorch Lightning\n",
        "\n",
        "<img src=\"https://warehouse-camo.ingress.cmh1.psfhosted.org/a88de56e65d2ea6bc203ce178a1cecbe9b50a0ac/68747470733a2f2f6769746875622e636f6d2f5079546f7263684c696768746e696e672f7079746f7263682d6c696768746e696e672f7261772f312e342e392f646f63732f736f757263652f5f7374617469632f696d616765732f6c6f676f2e706e67\">\n",
        "\n",
        "\n",
        "Pytorch lightning will handle a lot of things for you. It creates a Trainer which is a Code Management trick used by many companies (Meta, Google..) in order to get much more digest code.\n",
        "\n",
        "\n",
        "More Information on : https://www.pytorchlightning.ai/\n",
        "\n",
        "Goal of this lab :\n",
        "\n",
        "* Use Pytorch Lightning for Training\n",
        "* Learn to use Pytorch-Lightning\n",
        "* Do classification on MNIST, CIFAR-10\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_cnXBS7boC7"
      },
      "source": [
        "# I - Classify Numbers using Lightning\n",
        "\n",
        "In this part, we are interested in classifying digit images ranging from 0 to 9. \n",
        "We will use the Lightning framework for code management. What's interesting about Lightning is that you can plug in your Torch modules without any modification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xBCfzUJcJO9"
      },
      "source": [
        "## a - LightningDataModule : MNIST\n",
        "\n",
        "As you have seen in the Tutorial, you need to create your Dataset Class.\n",
        "\n",
        "As a reminder :    \n",
        " The Dataset class returns one sample of your dataset at a time. The main methods of the Dataset class are \n",
        "\n",
        "*   __getitem__ : which fetched a sample at a given index\n",
        "*   __len__ : which returns the len of the total dataset\n",
        "\n",
        "The Dataset is loaded into a DataLoader. That Dataloader is then used to **fetch and send data as batches** for your Model.\n",
        "\n",
        "You will see that using Lightning makes things clearer. LightningDataModule allows you to write cleaner Code and fit easily your data to your model.\n",
        "\n",
        "You can always, use the basic Pytorch Dataloader in a separate code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaDUVAFvqp5h"
      },
      "source": [
        "We are first going to work with the MNIST dataset. There is already a MNIST class provided in the Torchvision library, so we don't have to code the Dataset implementation ourself.\n",
        "* Fill in the blanks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrxvDv_8r9mU"
      },
      "source": [
        "### Exploratory Data Analysis : Discovering the Data\n",
        "\n",
        "Before tackling the classification task it is essential to explore the data we are working on, and understand its specifics.\n",
        "Perform an Exploratory Data Analysis (EDA) on the MNIST Dataset :\n",
        "\n",
        "1.   What type of Data do you have ? (Images, Texts, Sound..)\n",
        "2.   How many Data do you have ? \n",
        "3.   What's in a sample (1 element of the Dataset)\n",
        "4.   Is the Dataset umbalanced ?\n",
        "5.   What's the shape of any input sample ?\n",
        "6.   ....\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDrwvhJAsEz_"
      },
      "outputs": [],
      "source": [
        "# Loading the Training Split of MNIST Dataset\n",
        "dataset_train  = MNIST('', train=True, download=True)\n",
        "dataset_test = MNIST('', train=False, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vwYTFrhtZ43",
        "outputId": "f3074ec1-6863-4aa4-aac6-540ae2ade880"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO : What's the length of the train and test split ?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF-0t0ETt1Fa",
        "outputId": "d8fbc436-4601-4d53-cee5-e046526f8522"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<PIL.Image.Image image mode=L size=28x28 at 0x7F42DD677990>, 5)\n"
          ]
        }
      ],
      "source": [
        "# TODO : Retrieve one sample of the Dataset.\n",
        "sample = dataset[...]\n",
        "\n",
        "# TODO : What is in a sample ? Print the sample to understand\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pNqZ8DmhuDU0"
      },
      "outputs": [],
      "source": [
        "# TODO : Plot the image in the sample. Does it correspond to the second element of the sample ?\n",
        "\n",
        "plt.plot(...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUYEXO0TuZ24"
      },
      "outputs": [],
      "source": [
        "# TODO : What's the shape of the input image.\n",
        "shape = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBDDrYrKlr8X"
      },
      "source": [
        "### Lightning DataModule : Dataset and DataLoader Embedded \n",
        "\n",
        "Pytorch Lightning introduces a new way to define and organize our dataset via \"data module\". They neatly encompass our training, validation and testing datasets and provide their dataloaders as well.\n",
        "\n",
        "Have a look at : https://pytorch.org/vision/stable/datasets.html\n",
        "\n",
        "We are now going to define a `LightningDataModule` for the MNIST dataset. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HsUhr7BihqxX"
      },
      "outputs": [],
      "source": [
        "class MNISTDataModule(pl.LightningDataModule):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "        self.data_dir = ''\n",
        "        self.batch_size_train, self.batch_size_valid, self.batch_size_test = 32,32,32\n",
        "\n",
        "    def prepare_data(self):\n",
        "        # This method is used to download beforehand the dataset if needed.\n",
        "        # TODO : Load the train and test dataset.\n",
        "        MNIST(self.data_dir, train=..., download=...)\n",
        "        \n",
        "\n",
        "    def setup(self, stage):\n",
        "        # We need to setup our module. We have \n",
        "        #  1. A training set that we will **fit** our model to\n",
        "        #  2. A testing set used to **test** our models prediction.\n",
        "        \n",
        "        # The stage variable corresponds to those two steps : \n",
        "        # stage in {fit, test, None}\n",
        "\n",
        "        # First stage is 'fit' (or None)\n",
        "        if stage == \"fit\" or stage is None:\n",
        "            # We create a validation split to watch the training.\n",
        "\n",
        "            # TODO : Which dataset do we load for training ?\n",
        "            mnist_dataset = MNIST(self.data_dir, train=..., transform=self.transform)\n",
        "            train_size = int(0.8 * len(mnist_dataset))\n",
        "            test_size = len(dataset_train) - train_size\n",
        "            mnist_train, mnist_valid =  torch.utils.data.random_split(mnist_dataset, [train_size, test_size])\n",
        "            \n",
        "            # TODO : Load the datasets as attributes of the Module. Don't forget you validation split\n",
        "            self.mnist_train, _ = \n",
        "\n",
        "        # Second stage is 'test' \n",
        "        if stage == \"test\" or stage is None:\n",
        "\n",
        "            self.mnist_test = MNIST(self.data_dir, train=False, transform=self.transform)\n",
        "            # Question : What additional set can we create ? Why ?\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        # TODO : Now create your Training DataLoader\n",
        "        return ...\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        # TODO : Now create your Validation DataLoader\n",
        "        return ...\n",
        "\n",
        "    def test_dataloader(self):\n",
        "        # TODO : Now create your Testing DataLoader\n",
        "        return ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71GzlDYqj5xY"
      },
      "source": [
        "## b - LightningModule :  MNIST Classifier \n",
        "\n",
        "Similarly to `LightningDataModule`, pytorch Lightning provides a `LightningModule` that gathers every functions needed for the training and testing of our pytorch model.\n",
        " \n",
        "Design a model to perform Classification using the `LightningModule` class layout. Again, ask yourself the following questions: \n",
        "* What task is it ?\n",
        "* What data do I have ?\n",
        "* What learning rate should I use ?\n",
        "* What could be my loss ? Why ?\n",
        "* What non-linearity should I use ?\n",
        "* How do I evaluate my model ? (TorchMetrics is your friend)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUBuWwX3iypq"
      },
      "outputs": [],
      "source": [
        "class MNISTClassifier(pl.LightningModule):\n",
        "    def __init__(self, output_shape):\n",
        "        super(MNISTClassifier,self).__init__()\n",
        "        # what is the output_shape of your model ?\n",
        "        self.output_shape = output_shape\n",
        "        self.save_hyperparameters()\n",
        "        # TODO : Define your model here, be careful, your model will be an instance of the class. Watch  out for the input data.\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        # TODO : What would be the forward steps of this classifier ? Return the output of our classifier given an input batch x.\n",
        "        ...\n",
        "        return x\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # TODO : Choose your optimizer : https://pytorch.org/docs/stable/optim.html\n",
        "        optimizer = ...\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # TODO : Define your Training Step\n",
        "        # This method is pretty much similar to what your did in the Tutorial to train your model.\n",
        "        x,y = batch     \n",
        "        ...\n",
        "        loss = \n",
        "        acc = \n",
        "        # Don't remove the next line, you will understand why later\n",
        "        self.log('train_acc', acc)\n",
        "        self.log('train_loss', loss)\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # TODO : Define your Validation Step\n",
        "        # What is the difference between the Training and the Validation Step ?\n",
        "        x,y = batch\n",
        "        ...\n",
        "        loss = ...\n",
        "        acc = ...\n",
        "        # Don't remove the next line, you will understand why later\n",
        "        self.log('val_acc', acc)\n",
        "        self.log('val_loss', loss)\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        # TODO : Define your Test Step\n",
        "        # What is the difference between the Training, Validation and Test Step ?\n",
        "        x,y = batch\n",
        "        ...\n",
        "        loss = ...\n",
        "        self.acc = ... # We accumulate every accuracy\n",
        "        # Don't remove the next line, you will understand why later\n",
        "        self.log('test_loss', loss)\n",
        "        self.log('test_acc', self.acc)\n",
        "\n",
        "    def test_epoch_start(self):\n",
        "        self.acc = 0\n",
        "\n",
        "    def test_epoch_end(self):\n",
        "        self.acc = \n",
        "        self.log('Final Accuracy', self.acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TV-SLU6kWV7"
      },
      "source": [
        "## c - Did you say Train ?\n",
        "\n",
        "Let's train the model. \n",
        "\n",
        "We create our so called Trainer that will handle a lot of thing for us. Lightning trainer is full of interesting assets that helps you for your training. The lightning trainer is a much more evolved Trainer than the one in the Tutorial.\n",
        "\n",
        "To get a glance of what Lightning Trainer can give :\n",
        "https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html\n",
        "\n",
        "It also easily lets us using TensorBoard. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9wyr4pjj5R-"
      },
      "outputs": [],
      "source": [
        "tb_logger = pl_loggers.TensorBoardLogger(\"introduction to Lightning\")\n",
        "\n",
        "dm = MNISTDataModule()\n",
        "model = MNISTClassifier(10)\n",
        "\n",
        "trainer = pl.Trainer(gpus=-1,max_epochs=10,accelerator='dp',logger=tb_logger)\n",
        "trainer.fit(model, dm)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ej56Kf9Zkefo"
      },
      "source": [
        "Oh it's training ! Happy ? Easy ? Let's test the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaWmGYVTjRz5"
      },
      "source": [
        "## d - Did you say Test ?\n",
        "\n",
        "For testing, well it's pretty easy "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idVeeYZKVvSS"
      },
      "outputs": [],
      "source": [
        "trainer.test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO_fuzVPMNX5"
      },
      "source": [
        "## e - TensorBoard\n",
        "\n",
        "TensorBoard is a really useful tool. Indeed, it let's you register interesting values during training and plot them INTERACTIVELY. You might have seen a self.log line in the Validation and Training steps. \n",
        "The self.log saves the loss value into a TensorBoard readable file. We can also add images or other values using self.log\n",
        "\n",
        "In fact, look at the checkpoint created by the training. You might see 3 files :\n",
        "* Checkpoint\n",
        "* event.out....\n",
        "* hparam.yaml\n",
        "\n",
        "Let's open tensorboard to see how the training was. Tensorboard is loadable using magic_python commands.\n",
        "More info on TensorBoard : https://www.tensorflow.org/tensorboard/get_started\n",
        "\n",
        "Another popular alternative to Tensorboard, also usable with pytorch lightning, is \"Weight and Biases\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzLzFpFdkdoM"
      },
      "outputs": [],
      "source": [
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir \"/content/introduction to Lightning/default/version_0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCx4XtQojqBf"
      },
      "source": [
        "Pytorch Lightning can be used along PyTorch. We encourage you to use PyTorch Lightning during your Lab Sessions and Career as it simplifies a lot of things for you (MultiGPU, Learning Rate Decay...)\n",
        "\n",
        "<img src='https://c.tenor.com/VyApQ-jWyV0AAAAC/happy-borat.gif'>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rq2vIzXbyXtk"
      },
      "source": [
        "# II - Classify Objects using Lightning \n",
        "\n",
        "We will now turn to another classification task, this time object classification. We use the CIFAR-10 dataset which is made of 10 different classes of objects which we aim to distinguish. This part of the lab will be less restricted and more free. You now should have a sense of how to use the Lightning Framework. \n",
        "Be creative.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVi6aj84y97g"
      },
      "source": [
        "## a - Baseline : Creating your own Model\n",
        "\n",
        "In this first section, we will create a Simple Model and perform all steps from part 1 with the needed changes.\n",
        "\n",
        "*   **What's your final accuracy ?**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LodackDWzb9k"
      },
      "source": [
        "### i - DataModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "La8kvImxzgaf"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUes-3TPFhW8"
      },
      "outputs": [],
      "source": [
        "# TODO : EDA "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hJKuGjwFHxE"
      },
      "outputs": [],
      "source": [
        "# TODO : Create your DataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ykxzmaMzhEr"
      },
      "source": [
        "### ii - Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZWj3Qp8zgej"
      },
      "outputs": [],
      "source": [
        "# TODO : Create your Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjDUQJBhzloB"
      },
      "source": [
        "### iii - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1-7v8j9zoAJ"
      },
      "outputs": [],
      "source": [
        "# TODO : Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8RmPe1ozoxq"
      },
      "source": [
        "### iv - Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXDiuQOMzoxr"
      },
      "outputs": [],
      "source": [
        "# TODO : Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ls7EK0Az2YFr"
      },
      "source": [
        "\n",
        "**Does your model perform well on the CIFAR Dataset ?**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hfcW6jcizB1I"
      },
      "source": [
        "## b - The OG Model : Finetuning a Model\n",
        "\n",
        "If your model performed well on the CIFAR-10 Dataset, congrats. But let's achieve better results. For industrial works, we often pretrain a model on a large dataset (ImageNet or internal Dataset), and then fine-tune the model on the Dataset of interest.\n",
        "\n",
        "* **What's the intuition behind fine-tuning ?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qxg-fff7Ofw"
      },
      "source": [
        "### i - Importing a Pretrained Model\n",
        "\n",
        "We will import a ConvNext model. Why ? It's said to be a really good backbone that competes with the Transformer models. Let's load the model. \n",
        "We are going to use TorchSummary to print what the size of the inputs and outputs are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIgUs9Fd7FW7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torchvision\n",
        "model = torchvision.models.convnext_small(weights='DEFAULT')\n",
        "\n",
        "# TODO : Using torchsummary, print a summary of the model\n",
        "from torchsummary import summary\n",
        "summary(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x29gjtvd94Jr"
      },
      "outputs": [],
      "source": [
        "# TODO : Using torchsummary, send an image of the same size as a sample of CIFAR-10\n",
        "summary(model, ...) # ... = input shape as a tuple (C,H,W)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIpoh9qP-Nll"
      },
      "source": [
        "* **What is the output size of the model ?**\n",
        "* **What will be the issue of using this model as is to perform classification on the CIFAR-10 Dataset ?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XBym9hsB2Z1"
      },
      "outputs": [],
      "source": [
        "# TODO : According to your answer to the previous questions, perform the changes.\n",
        "# You can access each layers using model.name_of_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcainrOT51rP"
      },
      "source": [
        "### ii - DataModule"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3WlWJvy51rP"
      },
      "outputs": [],
      "source": [
        "# TODO : EDA \n",
        "\n",
        "# TODO : Create your DataModule"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aUiAPtd51rP"
      },
      "source": [
        "### iii - Module\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0zUy7CJ51rP"
      },
      "outputs": [],
      "source": [
        "# TODO : Create your Module\n",
        "\n",
        "# Careful : How should your learning rate be ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwPYkPT551rQ"
      },
      "source": [
        "### iv - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GdvMUH-51rQ"
      },
      "outputs": [],
      "source": [
        "# TODO : Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEJLdQuZ51rQ"
      },
      "source": [
        "### v - Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvxjzGdV51rQ"
      },
      "outputs": [],
      "source": [
        "# TODO : Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "USu9oo2wCV2j"
      },
      "source": [
        "\n",
        "* **What is your final accuracy ?**\n",
        "* **Is Fine Tuning a model better than creating your own model ?**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Introduction to Pytorch Lightning",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
